############################################################################
#
# SCENARIO 8: Snapshots management with ONTAP-NAS & ONTAP-NAS-ECONOMY
#
############################################################################

GOAL:
This scenario can be used to demonstrate how snapshots can be in a NAS context, and discover the caveats to apply when using Qtrees.

The files used in the scenario are all in the directory: https://github.com/YvosOnTheHub/LabNetApp/tree/master/Kubernetes/Scenarios/Scenario8.
The POD definition used 3 PVC:
- 1 FlexVol
- 2 Qtrees

Prerequisites:
- Trident v19.04 is already installed & configured
- Storage Classes should limit the backend one single aggregate, so that we are sure that the Qtree PVC share the same FlexVol
- Backends and StorageClass are already configured

[root@rhel3 Kubernetes]# tridentctl -n trident get backend
+---------------------+-------------------+--------+---------+
|        NAME         |  STORAGE DRIVER   | STATE  | VOLUMES |
+---------------------+-------------------+--------+---------+
| ECO_with_snapdir_on | ontap-nas-economy | online |       0 |
| NAS_with_snapdir_on | ontap-nas         | online |       0 |
+---------------------+-------------------+--------+---------+

[root@rhel3 Kubernetes]# kubectl get sc
NAME                      PROVISIONER         AGE
storage-class-nas         netapp.io/trident   1d
storage-class-nas-qtree   netapp.io/trident   1d


A. Let's prepare the environment and create 3 PVC (1 FlexVol & 2 Qtrees)

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas.yaml
persistentvolumeclaim "pvc-nas" created

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas-qtree.yaml
persistentvolumeclaim "pvc-nas-qtree" created

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas-qtree2.yaml
persistentvolumeclaim "pvc-nas-qtree2" created

[root@rhel3 Kubernetes]# kubectl get pvc
NAME                                   STATUS    VOLUME                         CAPACITY     ACCESS MODES   STORAGECLASS              AGE
persistentvolumeclaim/pvc-nas          Bound     default-pvc-nas-94c7a          1073741824   RWO            storage-class-nas         22s
persistentvolumeclaim/pvc-nas-qtree    Bound     default-pvc-nas-qtree-97e30    1073741824   RWO            storage-class-nas-qtree   17s
persistentvolumeclaim/pvc-nas-qtree2   Bound     default-pvc-nas-qtree2-997c5   1073741824   RWO            storage-class-nas-qtree   14s

[root@rhel3 Kubernetes]# tridentctl -n trident get backend
+---------------------+-------------------+--------+---------+
|        NAME         |  STORAGE DRIVER   | STATE  | VOLUMES |
+---------------------+-------------------+--------+---------+
| ECO_with_snapdir_on | ontap-nas-economy | online |       2 |
| NAS_with_snapdir_on | ontap-nas         | online |       1 |
+---------------------+-------------------+--------+---------+


B. We can now create a simple app that will mount the 3 PVC

[root@rhel3 Kubernetes]# kubectl create -f pod-alpine-nas_snaptest.yaml
pod "podsnap" created

Looking at the pod description, we can see that the volumes are indeed mounted:

[root@rhel3 Kubernetes]# kubectl describe pod podsnap
Name:         podsnap
...
Mounts:
      /data_qtree1 from qtree1 (rw)
      /data_qtree2 from qtree2 (rw)
      /data_vol from flexvol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hb5km (ro)
...

the following will tell us how many mounts are done on the SVM we are using (we should get 3 as a result)
[root@rhel3 Kubernetes]# kubectl exec -it podsnap -- mount | grep 192.168.0.135 | wc -l
3


C. Let's create a file in each mount

[root@rhel3 Kubernetes]# kubectl exec -it podsnap -- touch /data_vol/file_vol.txt
[root@rhel3 Kubernetes]# kubectl exec -it podsnap -- touch /data_qtree1/file_qtree1.txt
[root@rhel3 Kubernetes]# kubectl exec -it podsnap -- touch /data_qtree2/file_qtree2.txt


D. Let's create snapshots on the storage backend

In our example:
- the snapshot for the volume linked to the PVC data_vol will be called snap_vol
- the snapshot for the volume hosting both qtrees will be called snap_qtree


E. Let's check we do have access to the snapshots from within the pod

E.1 PVC DATA_VOL
[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_vol/
...
drwxrwxrwx    3 root     root          4096 Jul  8 12:46 .snapshot
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_vol.txt

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_vol/.snapshot/
...
drwxrwxrwx    2 root     root          4096 Jul  8 12:43 snap_vol1

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_vol/.snapshot/snap_vol1
...
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_vol.txt

E.2 PVC DATA_QTREE1
[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree1
...
drwxrwxrwx    3 root     root          4096 Jul  8 12:46 .snapshot
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_qtree1.txt

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree1/.snapshot
...
drwxrwxrwx    2 root     root          4096 Jul  8 12:43 snap_qtree

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree1/.snapshot/snap_qtree
...
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_qtree1.txt

E.3 PVC DATA_QTREE1
[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree2
...
drwxrwxrwx    3 root     root          4096 Jul  8 12:46 .snapshot
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_qtree2.txt

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree2/.snapshot
...
drwxrwxrwx    2 root     root          4096 Jul  8 12:43 snap_qtree

[root@rhel3 Kubernetes]# k exec -it podsnap -- ls -la /data_qtree2/.snapshot/snap_qtree
...
-rw-r--r--    1 root     root             0 Jul  8 12:43 file_qtree2.txt



