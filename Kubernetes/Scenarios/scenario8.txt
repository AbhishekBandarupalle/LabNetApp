############################################################################
#
# SCENARIO 8: Snapshots management with ONTAP-NAS & ONTAP-NAS-ECONOMY
#
############################################################################

GOAL:
This scenario can be used to demonstrate how snapshots can be in a NAS context, and discover the caveats to apply when using Qtrees.

The files used in the scenario are all in the directory: https://github.com/YvosOnTheHub/LabNetApp/tree/master/Kubernetes/Scenarios/Scenario8.
The POD definition used 3 PVC:
- 1 FlexVol
- 2 Qtrees

Prerequisites:
- Trident v19.04 is already installed & configured
- Storage Classes should limit the backend one single aggregate, so that we are sure that the Qtree PVC share the same FlexVol
- Backends and StorageClass are already configured

[root@rhel3 Kubernetes]# tridentctl -n trident get backend
+---------------------+-------------------+--------+---------+
|        NAME         |  STORAGE DRIVER   | STATE  | VOLUMES |
+---------------------+-------------------+--------+---------+
| ECO_with_snapdir_on | ontap-nas-economy | online |       0 |
| NAS_with_snapdir_on | ontap-nas         | online |       0 |
+---------------------+-------------------+--------+---------+

[root@rhel3 Kubernetes]# kubectl get sc
NAME                      PROVISIONER         AGE
storage-class-nas         netapp.io/trident   1d
storage-class-nas-qtree   netapp.io/trident   1d


A. Let's prepare the environment and create 3 PVC (1 FlexVol & 2 Qtrees)

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas.yaml
persistentvolumeclaim "pvc-nas" created

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas-qtree.yaml
persistentvolumeclaim "pvc-nas-qtree" created

[root@rhel3 Kubernetes]# kubectl create -f pvc-nas-qtree2.yaml
persistentvolumeclaim "pvc-nas-qtree2" created

[root@rhel3 Kubernetes]# kubectl get pvc
NAME                                   STATUS    VOLUME                         CAPACITY     ACCESS MODES   STORAGECLASS              AGE
persistentvolumeclaim/pvc-nas          Bound     default-pvc-nas-94c7a          1073741824   RWO            storage-class-nas         22s
persistentvolumeclaim/pvc-nas-qtree    Bound     default-pvc-nas-qtree-97e30    1073741824   RWO            storage-class-nas-qtree   17s
persistentvolumeclaim/pvc-nas-qtree2   Bound     default-pvc-nas-qtree2-997c5   1073741824   RWO            storage-class-nas-qtree   14s

[root@rhel3 Kubernetes]# tridentctl -n trident get backend
+---------------------+-------------------+--------+---------+
|        NAME         |  STORAGE DRIVER   | STATE  | VOLUMES |
+---------------------+-------------------+--------+---------+
| ECO_with_snapdir_on | ontap-nas-economy | online |       2 |
| NAS_with_snapdir_on | ontap-nas         | online |       1 |
+---------------------+-------------------+--------+---------+


B. We can now create a simple app that will mount the 3 PVC

[root@rhel3 Kubernetes]# kubectl create -f pod-alpine-nas_snaptest.yaml
pod "podsnap" created

Looking at the pod description, we can see that the volumes are indeed mounted:

[root@rhel3 Kubernetes]# kubectl describe pod podsnap
Name:         podsnap
...
Mounts:
      /data_qtree1 from qtree1 (rw)
      /data_qtree2 from qtree2 (rw)
      /data_vol from flexvol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hb5km (ro)
...

the following will tell us how many mounts are done on the SVM we are using (we should get 3 as a result)
[root@rhel3 Kubernetes]# k exec -it podsnap -- mount | grep 192.168.0.135 | wc -l
3



